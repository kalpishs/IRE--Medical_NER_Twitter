{
  "name": "Ire--medical ner twitter",
  "tagline": "",
  "body": "____________________________________________________________________________________________________________________________________________\r\nMentor: Nikhil Pattisapu\r\n\r\nProject No. : 19\r\n\r\nTeam No. : 56\r\n\r\nBy Chinmay Bapna, Juhi Tandon, Kalpish Singhal\r\n____________________________________________________________________________________________________________________________________________\r\n\r\n==============================================================================================================================================\r\n Medical Named Entity on twitter Data\r\n==============================================================================================================================================\r\nMedical Entity Recognition is a crucial step towards efficient medical texts analysis. The task of a Medical Name Entity Recognizer is two fold. ­ \r\n\r\n(i) Identification of entity boundaries in the sentences. \r\n\r\n(ii) Entity categorization. \r\nOur objective is to extend medical entity  recognition for tweets. \r\n\r\nMedical   entities   can   be   diseases,   drugs,   symptoms,   etc.   Previously,   researchers   in   the   field  have   used   hand   crafted   features   to   identify   medical   entities   in   medical   literature.   It   has   been  found   that   in   contrast   with   semantic   approaches   which   require   rich   domain­ knowledge for rule or pattern construction, statistical approaches are more scalable.\r\n\r\n\r\n==============================================================================================================================================\r\nData set Used\r\n==============================================================================================================================================\r\nTwitter data SET for Training and Testing purposes\r\n \tWe have a dataset of 1 year of tweets about 4 diseases and 32 drugs. A team of domain  experts has annotated about 2000 tweets with entities (around 20 types: diseases, drugs,  symptoms) and also relations (around 40 relation types: cures, causes, etc). \r\n\r\n==============================================================================================================================================\r\nProject Scope\r\n==============================================================================================================================================\r\nThis project is mainly focused on feature extraction  of medical entities on the set of tweets. This will be divided into three phases namely.\r\n\r\n\t(i) Feature Identification and extraction\r\n\t\r\n\t(ii) Model training using CRF \r\n\t\r\n\t(iii) Testing using trained model\r\n\t\r\n\t(iv) Evaluating the output obtained for different feature-models using metrics such as Precision, Recall, F-score and Accuracy.\r\n\t\r\n\t(v) Selecting some feature models and plot their precision , recall , F-scores for each label\r\n\r\n\t\r\n==============================================================================================================================================\r\nList of Files \r\n==============================================================================================================================================\r\n\r\n(i) bash_5gram.sh (Bash script to execute the project files in one-go)\r\n\r\n(ii) training_features_5gram.py (main code for constructing feature files)\r\n\r\n(iii) metamap.py (for extracting semantic tags from Metamap(tool) generated output)\r\n\r\n(iv) pos_tagger.py (for extracting part-of-speech tags from Tweet-NLP(tool) generated output)\r\n\r\n(v) ortho.py (for assigning orthographic features)\r\n\r\n(vi) cluster.py (for extracting cluster-id tags from Brown-Clustering(algorithm) generated output)\r\n\r\n(vii) part.sh (for testing-exploring new feature sets from pre-existing feature file)\r\n\r\n(viii) evaluate.py (for testing system generated output on the basis of different evaluation metrics)\r\n\r\n\r\n==============================================================================================================================================\r\nHow to Run  \r\n==============================================================================================================================================\r\n->Install Following Tools:\r\n\t\r\n1)Mallet :- A Java-based package for statistical natural language processing,text classification and  information extraction tool using Command line scripts.\r\n  MALLET includes sophisticated tools for document classification: efficient routines for converting text to \"features\",a wide variety of algorithms eg. CRF\r\n           Installation instructions available at http://mallet.cs.umass.edu/\r\n\r\n2)Meta-Map 2013 :- MetaMap is a highly configurable program developed to map biomedical text to the UMLS Metathesaurus or,equivalently, to discover Metathesaurus concepts referred to in text. We have made use of Meta-Map Java-api for making use of this tool to find semantic tags.\r\n\t  Installation instructions available at https://metamap.nlm.nih.gov/JavaApi.shtml\r\n\r\n3)Tweet-NLP :-  Provide a tokenizer, a part-of-speech tagger, hierarchical word clusters, and a dependency parser for tweet. We used it to address the problem of part-of-speech tagging for English data from the popular micro blogging service Twitter. The tool reports tagging results nearing 90% accuracy.\r\n    Installation instructions available at http://www.cs.cmu.edu/~ark/TweetNLP/\r\n\r\n4)You'll also need to run Brown-CLustering algorithm on both - testing and training data, for this project, we already have stored its output in usable format in the file IRE--Medical_NER_Twitter/data/cluster-50/paths Code and usage instructions available at https://github.com/percyliang/brown-cluster\r\n \r\nDownload the Project from here, https://github.com/kalpishs/IRE--Medical_NER_Twitter.git\r\nConfigure the file bash_5gram.sh in the codes_Phase2 folder in src, with the paths of installed tools, data, etc.\r\nStart the metamap server, for Metamap server Java-api, run bash publc-mm/bin/mmserver13  (public-mm -> Metamap installed directory) \r\nExecute bash_5gram.sh and voila -> it will run successfully.\r\nThis bash script calls, \r\nAll the files mentioned in the previous section sequentially, It also runs commands to get output from different tools such as:\r\n\r\n1) bash $path/testapi.sh --input $files --output meta_out      (Running metamap)\r\n\r\n2) bash $path2/runTagger.sh $files) > pos_out                  (Running tweet-nlp)\r\n\r\n3) java -cp $java_home/class:$java_home/lib/mallet-deps.jar cc.mallet.fst.SimpleTagger --train true --model-file ../models/\r\n   trained_model_5gram ../training_files/training_file_5gram   (Running mallet on training data and training model 'trained_model_5gram')\r\n\r\n4) java -cp $java_home/class:$java_home/lib/mallet-deps.jar cc.mallet.fst.SimpleTagger --model-file ../models/trained_model_5gram \r\n   ../testing_files/testing_file_5gram) > ../system_result/system_tags_5gram  \t(Running mallet on testing data using trained model)\r\n\r\n\r\n==============================================================================================================================================\r\nOutput FORMAT\r\n==============================================================================================================================================\r\nWe have used the BIO(Begin-Inside-Outside)  model for labelling entities. For a whole corpus of tweets, we are labelling data term-wise. This model (BIO) lets us handle entities that span over multiple terms.\r\n\r\nFor instance, the words “childhood asthma” represent a single entity which is a disease, In accordance with our  implementation, the separate words childhood and asthma will be labelled as\r\n\r\nchildhood -> Disease-Begin  and asthma -> Disease-Inside.\r\n\r\nSo, this is the output format and the corresponding labels are as follows:\r\n\r\n{Disease-begin, Disease-inside, Drug-begin, Drug-inside, Symptom-begin, Symptom-inside, None}\r\n\r\nSo, we basically assign 2 labels (Begin, Inside) per entity (Disease, Drug, Symptom) and 1 extra label (None) representing Outside tag in BIO model.\r\n\r\nThese labels are the output which are obtained in system generated file by the trained tool.\r\n\r\n\r\n==============================================================================================================================================\r\nConclusion\r\n==============================================================================================================================================\r\nThe dataset provided to us consists of 85% of ‘None’ entity tags, hence if we label all the terms as none - we’ll have 85% accuracy. This conveys that accuracy is not the correct metric for correct evaluation of feature models.\r\n\r\nSince this metric isn’t good-enough to tell us significance of one feature model over another, we tried using more application specific metrics such as Precision, Recall and F-Score. \r\n\r\nIn this project we experimented with different feature sets and evaluated their efficiency in NER.\r\n\r\n--- The best 3 feature models along with statistic analysis are represented in the presentation available at: <https://goo.gl/GN0AWr>\r\n\r\n--- A video demonstrating our work has also been put-up and can be accessesed from: <https://youtu.be/dFKIy7CgMrg>\r\n\r\n---Dropbox link :- <https://goo.gl/3Plc8s> \r\n\r\n---Slideshare: http://www.slideshare.net/NitishJain24/ire-presentation-team56\r\n\r\n\r\n==============================================================================================================================================\r\nTags\r\n==============================================================================================================================================\r\n'Information Retrieval and Extraction Course', 'IIIT-H', Major Project', 'Mallet', 'Medical NER', 'Feature Sets', 'Disease', 'Drug', 'Symptom', 'Analysis and Approach'\r\n\r\n\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}